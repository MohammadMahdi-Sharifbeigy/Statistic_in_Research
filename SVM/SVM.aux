\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}ماشین بردار پشتیبان (SVM) چیست؟}{3}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{3}{ماشین بردار پشتیبان (SVM) چیست؟}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}جداکننده خطی و مفهوم حاشیه بیشینه}{3}{section.2}\protected@file@percent }
\newlabel{sec:max_margin}{{2}{3}{جداکننده خطی و مفهوم حاشیه بیشینه}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces چندین خط می‌توانند دو کلاس را از هم جدا کنند، اما کدام یک بهترین است؟}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:hyperplanes}{{1}{3}{چندین خط می‌توانند دو کلاس را از هم جدا کنند، اما کدام یک بهترین است؟}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces ابرصفحه با حاشیه بیشینه. نقاطی که روی خطوط حاشیه قرار دارند، \textbf  {بردارهای پشتیبان} \lr {(Support Vectors)} نامیده می‌شوند.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:max_margin}{{2}{4}{ابرصفحه با حاشیه بیشینه. نقاطی که روی خطوط حاشیه قرار دارند، \textbf {بردارهای پشتیبان} \lr {(Support Vectors)} نامیده می‌شوند}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}مقابله با داده‌های جداناپذیر: حاشیه نرم}{4}{section.3}\protected@file@percent }
\newlabel{sec:soft_margin}{{3}{4}{مقابله با داده‌های جداناپذیر: حاشیه نرم}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces در روش حاشیه نرم، به برخی نقاط اجازه داده می‌شود که حاشیه را نقض کنند (\lr {$\xi _i > 0$}). این نقاط متخلف نیز بردار پشتیبان محسوب می‌شوند.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:soft_margin}{{3}{5}{در روش حاشیه نرم، به برخی نقاط اجازه داده می‌شود که حاشیه را نقض کنند (\lr {$\xi _i > 0$}). این نقاط متخلف نیز بردار پشتیبان محسوب می‌شوند}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}تابع هزینه Hinge Loss}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}ریاضیات بهینه‌سازی: لاگرانژ، دوگانگی و KKT}{6}{section.4}\protected@file@percent }
\newlabel{sec:math}{{4}{6}{ریاضیات بهینه‌سازی: لاگرانژ، دوگانگی و KKT}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}شرایط کاروش-کوهن-تاکر (KKT)}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}ترفند کرنل: حرکت به ابعاد بالاتر}{7}{section.5}\protected@file@percent }
\newlabel{sec:kernel}{{5}{7}{ترفند کرنل: حرکت به ابعاد بالاتر}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ایده اصلی ترفند کرنل: نگاشت داده‌ها به یک فضای با ابعاد بالاتر که در آن به صورت خطی جداپذیر شوند.}}{7}{figure.4}\protected@file@percent }
\newlabel{fig:kernel_trick}{{4}{7}{ایده اصلی ترفند کرنل: نگاشت داده‌ها به یک فضای با ابعاد بالاتر که در آن به صورت خطی جداپذیر شوند}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}انواع کرنل‌های متداول}{8}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}پرسش و پاسخ‌های عملی}{8}{section.6}\protected@file@percent }
\newlabel{sec:faq}{{6}{8}{پرسش و پاسخ‌های عملی}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}چرا SVM به مقیاس‌بندی ویژگی‌ها حساس است؟}{8}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}SVM چگونه مسائل چندکلاسه را حل می‌کند؟}{8}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}تأثیر پارامترهای C و Gamma چگونه است؟}{8}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}مقایسه فلسفه کرنل RBF و کرنل کسینوسی}{9}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}SVM در برابر نویز و داده‌های پرت چگونه عمل می‌کند؟}{9}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}نفرین ابعاد و بُعد VC چه هستند؟}{9}{subsection.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}نفرین ابعاد (\lr {Curse of Dimensionality})}{9}{subsubsection.6.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2}بُعد VC (\lr {Vapnik-Chervonenkis Dimension})}{10}{subsubsection.6.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}کاربرد ویژه: تشخیص داده پرت در الکتروفیزیولوژی}{10}{section.7}\protected@file@percent }
\newlabel{sec:outlier}{{7}{10}{کاربرد ویژه: تشخیص داده پرت در الکتروفیزیولوژی}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}الگوریتم \lr {One-Class SVM}}{10}{subsection.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces نمایش شماتیک عملکرد One-Class SVM. الگوریتم مرزی را دور داده‌های نرمال می‌آموزد تا نقاط خارج از آن را به عنوان ناهنجاری شناسایی کند.}}{10}{figure.5}\protected@file@percent }
\newlabel{fig:one_class_svm}{{5}{10}{نمایش شماتیک عملکرد One-Class SVM. الگوریتم مرزی را دور داده‌های نرمال می‌آموزد تا نقاط خارج از آن را به عنوان ناهنجاری شناسایی کند}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}تفاوت ریاضی با SVM استاندارد}{11}{subsubsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}مثال‌های عملی با Scikit-learn}{12}{subsubsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{مثال ۱: تشخیص داده‌های پرت ساختگی.}{12}{paragraph*.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces خروجی مثال Scikit-learn برای \lr {One-Class SVM}. شکل سمت چپ داده‌های نرمال (سفید) و پرت (نارنجی) شناسایی‌شده را نشان می‌دهد. شکل سمت راست، مرز تصمیم (خط سیاه) و ناحیه داده‌های نرمال (ناحیه رنگی) را به تصویر می‌کشد.}}{12}{figure.6}\protected@file@percent }
\newlabel{fig:sklearn_oneclass}{{6}{12}{خروجی مثال Scikit-learn برای \lr {One-Class SVM}. شکل سمت چپ داده‌های نرمال (سفید) و پرت (نارنجی) شناسایی‌شده را نشان می‌دهد. شکل سمت راست، مرز تصمیم (خط سیاه) و ناحیه داده‌های نرمال (ناحیه رنگی) را به تصویر می‌کشد}{figure.6}{}}
\@writefile{toc}{\contentsline {paragraph}{مثال ۲: مقایسه عملکرد روی توزیع‌های مختلف داده.}{12}{paragraph*.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces مقایسه عملکرد \lr {One-Class SVM} روی دو مجموعه داده متفاوت. در داده‌های سمت چپ که یک خوشه کلی را تشکیل می‌ده، یک مرز کلی ایجاد می‌شود. در داده‌های سمت راست که از دو خوشه مجزا تشکیل شده‌اند، مدل دو مرز جداگانه برای هر خوشه می‌آموزد.}}{13}{figure.7}\protected@file@percent }
\newlabel{fig:sklearn_comparison}{{7}{13}{مقایسه عملکرد \lr {One-Class SVM} روی دو مجموعه داده متفاوت. در داده‌های سمت چپ که یک خوشه کلی را تشکیل می‌ده، یک مرز کلی ایجاد می‌شود. در داده‌های سمت راست که از دو خوشه مجزا تشکیل شده‌اند، مدل دو مرز جداگانه برای هر خوشه می‌آموزد}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.3}مراحل عملی شناسایی آرتیفکت EEG با \lr {One-Class SVM}}{14}{subsubsection.7.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}جمع‌بندی: مزایا و معایب SVM}{15}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}مزایا}{15}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}معایب}{15}{subsection.8.2}\protected@file@percent }
\gdef \@abspage@last{15}
